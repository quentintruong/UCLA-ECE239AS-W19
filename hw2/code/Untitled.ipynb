{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Softmax(object):\n",
    "\n",
    "  def __init__(self, dims=[10, 3073]):\n",
    "    self.init_weights(dims=dims)\n",
    "\n",
    "  def init_weights(self, dims):\n",
    "    \"\"\"\n",
    "\tInitializes the weight matrix of the Softmax classifier.  \n",
    "\tNote that it has shape (C, D) where C is the number of \n",
    "\tclasses and D is the feature size.\n",
    "\t\"\"\"\n",
    "    self.W = np.random.normal(size=dims) * 0.0001\n",
    "\n",
    "  def loss(self, X, y):\n",
    "    \"\"\"\n",
    "    Calculates the softmax loss.\n",
    "  \n",
    "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "    of N examples.\n",
    "  \n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < C.\n",
    "  \n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the loss to zero.\n",
    "    loss = 0.0\n",
    "\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the normalized softmax loss.  Store it as the variable loss.\n",
    "    #   (That is, calculate the sum of the losses of all the training \n",
    "    #   set margins, and then normalize the loss by the number of \n",
    "    #   training examples.)\n",
    "    # ================================================================ #\n",
    "    for i in np.arange(X.shape[0]):\n",
    "      loss += np.log(np.sum(np.exp(np.matmul(self.W, X[i])))) - np.matmul(self.W[y[i]].transpose(), X[i])\n",
    "\n",
    "    loss /= X.shape[0]\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "\tSame as self.loss(X, y), except that it also returns the gradient.\n",
    "\n",
    "\tOutput: grad -- a matrix of the same dimensions as W containing \n",
    "\t\tthe gradient of the loss with respect to W.\n",
    "\t\"\"\"\n",
    "\n",
    "    # Initialize the loss and gradient to zero.\n",
    "    loss = 0.0\n",
    "    grad = np.zeros_like(self.W)\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and the gradient. Store the gradient\n",
    "    #   as the variable grad.\n",
    "    # ================================================================ #\n",
    "    for i in np.arange(X.shape[0]):\n",
    "      loss += np.log(np.sum(np.exp(np.matmul(self.W, X[i])))) - np.matmul(self.W[y[i]].transpose(), X[i])\n",
    "    loss /= X.shape[0]\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "      denominator = np.sum(np.exp(np.matmul(self.W, X[i])))\n",
    "      for j in range(self.W.shape[0]):\n",
    "        if j == y[i]:\n",
    "          grad[j] += -X[i] + np.exp(np.dot(self.W[j], X[i])) * X[i] / denominator\n",
    "        else:\n",
    "          grad[j] += np.exp(np.dot(self.W[j], X[i])) * X[i] / denominator\n",
    "    grad /= X.shape[0]\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def grad_check_sparse(self, X, y, your_grad, num_checks=10, h=1e-5):\n",
    "    \"\"\"\n",
    "    sample a few random elements and only return numerical\n",
    "    in these dimensions.\n",
    "    \"\"\"\n",
    "  \n",
    "    for i in np.arange(num_checks):\n",
    "      ix = tuple([np.random.randint(m) for m in self.W.shape])\n",
    "  \n",
    "      oldval = self.W[ix]\n",
    "      self.W[ix] = oldval + h # increment by h\n",
    "      fxph = self.loss(X, y)\n",
    "      self.W[ix] = oldval - h # decrement by h\n",
    "      fxmh = self.loss(X,y) # evaluate f(x - h)\n",
    "      self.W[ix] = oldval # reset\n",
    "  \n",
    "      grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "      grad_analytic = your_grad[ix]\n",
    "      rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "      print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "\n",
    "  def fast_loss_and_grad(self, X, y):\n",
    "    \"\"\"\n",
    "    A vectorized implementation of loss_and_grad. It shares the same\n",
    "\tinputs and ouptuts as loss_and_grad.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(self.W.shape) # initialize the gradient as zero\n",
    "  \n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Calculate the softmax loss and gradient WITHOUT any for loops.\n",
    "    # ================================================================ #\n",
    "    loss = np.sum(np.log(np.sum(np.exp(np.matmul(self.W, X.T)), axis=0)) - np.sum(self.W[y] * X, axis=1))\n",
    "    loss /= X.shape[0]\n",
    "    \n",
    "    base = np.exp(np.matmul(self.W, X.T)) / np.sum(np.exp(np.matmul(X, self.W.T)), axis=1)\n",
    "    print(base.shape)\n",
    "    y[0] = 12\n",
    "    base[y,np.arange(X.shape[0])] -= 1\n",
    "    print(base[y])\n",
    "    grad = np.matmul(base, X)\n",
    "    grad /= X.shape[0]\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "  def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "    \"\"\"\n",
    "    Train this linear classifier using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
    "      training samples each of dimension D.\n",
    "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
    "      means that X[i] has label 0 <= c < C for C classes.\n",
    "    - learning_rate: (float) learning rate for optimization.\n",
    "    - num_iters: (integer) number of steps to take when optimizing\n",
    "    - batch_size: (integer) number of training examples to use at each step.\n",
    "    - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "    Outputs:\n",
    "    A list containing the value of the loss function at each training iteration.\n",
    "    \"\"\"\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "\n",
    "    self.init_weights(dims=[np.max(y) + 1, X.shape[1]])\t# initializes the weights of self.W\n",
    "\n",
    "    # Run stochastic gradient descent to optimize W\n",
    "    loss_history = []\n",
    "\n",
    "    for it in np.arange(num_iters):\n",
    "      X_batch = None\n",
    "      y_batch = None\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Sample batch_size elements from the training data for use in \n",
    "      #      gradient descent.  After sampling,\n",
    "      #     - X_batch should have shape: (dim, batch_size)\n",
    "      #     - y_batch should have shape: (batch_size,)\n",
    "      #   The indices should be randomly generated to reduce correlations\n",
    "      #   in the dataset.  Use np.random.choice.  It's okay to sample with\n",
    "      #   replacement.\n",
    "      # ================================================================ #\n",
    "      indices = np.random.choice(X.shape[0], batch_size)\n",
    "      X_batch = X[indices,:]\n",
    "      y_batch = y[indices]\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      # evaluate loss and gradient\n",
    "      loss, grad = self.fast_loss_and_grad(X_batch, y_batch)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      # ================================================================ #\n",
    "      # YOUR CODE HERE:\n",
    "      #   Update the parameters, self.W, with a gradient step \n",
    "      # ================================================================ #\n",
    "      self.W -= learning_rate * grad\n",
    "\n",
    "      # ================================================================ #\n",
    "      # END YOUR CODE HERE\n",
    "      # ================================================================ #\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print('iteration {} / {}: loss {}'.format(it, num_iters, loss))\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    # ================================================================ #\n",
    "    # YOUR CODE HERE:\n",
    "    #   Predict the labels given the training data.\n",
    "    # ================================================================ #\n",
    "    y_pred = np.argsort(np.matmul(X, self.W.transpose()), axis=1)[:X.shape[0], self.W.shape[0] - 1]\n",
    "    # ================================================================ #\n",
    "    # END YOUR CODE HERE\n",
    "    # ================================================================ #\n",
    "\n",
    "    return y_pred\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
